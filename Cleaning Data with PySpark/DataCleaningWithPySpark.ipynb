{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e851de7-ff10-410a-a148-e1e0076d005b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1080c94d-12e4-44ca-995f-80e4f1e79c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#!{sys.executable} -m pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12e24cd6-cda4-4b61-b9b8-3941dd1266c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/20 10:27:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "\t.appName(\"DataCleaning\") \\\n",
    "\t.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0778f18-c8cf-4844-a8f3-6f7be43d4221",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# Function to generate random data with some missing values and duplicates\n",
    "def generate_data(n):\n",
    "    \n",
    "    customer_ids = [f'C{str(i).zfill(5)}' for i in range(1, 101)]\n",
    "    product_categories = ['Electronics', 'Books', 'Clothing', 'Groceries', 'Furniture']\n",
    "\n",
    "    data = []\n",
    "    for i in range(n):\n",
    "        customer_id = random.choice(customer_ids) if i % 10 != 0 else None  # Introduce some missing values\n",
    "        transaction_id = f'T{str(random.randint(10000, 99999))}'\n",
    "        transaction_date = pd.Timestamp('2023-01-01') + pd.to_timedelta(random.randint(0, 180), unit='d')\n",
    "        amount = round(random.uniform(5, 500), 2)\n",
    "        product_category = random.choice(product_categories)\n",
    "        data.append((customer_id, transaction_id, transaction_date, amount, product_category))\n",
    "\n",
    "        # Introduce duplicates\n",
    "        data.extend(data[:10])\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "924b3bd0-3a9c-4d48-85dc-6169b92e26ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------+------+---------------+\n",
      "|CustomerID|TransactionID|    TransactionDate|Amount|ProductCategory|\n",
      "+----------+-------------+-------------------+------+---------------+\n",
      "|      NULL|       T19714|2023-06-19 00:00:00|175.03|          Books|\n",
      "|      NULL|       T19714|2023-06-19 00:00:00|175.03|          Books|\n",
      "|    C00047|       T93197|2023-05-08 00:00:00| 62.47|          Books|\n",
      "|      NULL|       T19714|2023-06-19 00:00:00|175.03|          Books|\n",
      "|      NULL|       T19714|2023-06-19 00:00:00|175.03|          Books|\n",
      "+----------+-------------+-------------------+------+---------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <_io.BufferedWriter name=5>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/jupyterlab/4.4.5/libexec/lib/python3.13/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 200, in manager\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Generate 10,000 rows of data\n",
    "data = generate_data(10_000)\n",
    "\n",
    "columns = ['CustomerID', 'TransactionID', 'TransactionDate', 'Amount', 'ProductCategory']\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "spark_df = spark.createDataFrame(df)\n",
    "\n",
    "spark_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2368d51-813e-48a3-abd5-0c11b23af126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hanlde Missing values\n",
    "# Drop rows with missing CustomerID\n",
    "spark_df = spark_df.dropna(subset=[\"CustomerID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fefcea00-6c2b-43c7-ab70-3453aa9c5a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing CustomerID with a default value\n",
    "spark_df = spark_df.fillna({\"CustomerID\": \"Unknown\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c91546ea-c094-4a47-90b9-bb50ffeaced4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate rows based on 'TransactionID'\n",
    "spark_df = spark_df.dropDuplicates(subset=[\"TransactionID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c7893e5-3d61-4a2e-bd0c-ba7873fbb017",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize the ‘Amount’ column by scaling it between 0 and 1.\n",
    "from pyspark.sql.functions import col, min, max\n",
    "\n",
    "# Normalize the 'Amount' column\n",
    "min_amount = spark_df.agg(min(col(\"Amount\"))).collect()[0][0]\n",
    "max_amount = spark_df.agg(max(col(\"Amount\"))).collect()[0][0]\n",
    "\n",
    "spark_df = spark_df.withColumn(\"Amount\", (col(\"Amount\") - min_amount) / (max_amount - min_amount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b629d44b-4c8f-4b23-b03f-fa528088a6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle Outliers\n",
    "\n",
    "from pyspark.sql.functions import col, expr\n",
    "\n",
    "# Calculate Q1, Q3, and IQR\n",
    "quantiles = spark_df.approxQuantile(\"Amount\", [0.25, 0.75], 0.05)\n",
    "Q1 = quantiles[0]\n",
    "Q3 = quantiles[1]\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define the upper and lower bounds\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Filter out the outliers\n",
    "spark_df = spark_df.filter((col(\"Amount\") >= lower_bound) & (col(\"Amount\") <= upper_bound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ecf71dd-e5ae-4d50-8a75-ff946fc65def",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('CustomerID', 'string'),\n",
       " ('TransactionID', 'string'),\n",
       " ('TransactionDate', 'timestamp'),\n",
       " ('Amount', 'double'),\n",
       " ('ProductCategory', 'string')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ba59239-c044-4f14-a7e3-06fed12e2ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "# Convert 'TransactionDate' to date format\n",
    "spark_df = spark_df.withColumn(\"TransactionDate\", to_date(col(\"TransactionDate\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e3feea9-745f-460d-82a3-bfeaafc3c352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(CustomerID='C00041', TransactionID='T10009', TransactionDate=datetime.date(2023, 6, 17), Amount=0.6086306517435047, ProductCategory='Groceries'),\n",
       " Row(CustomerID='C00065', TransactionID='T10013', TransactionDate=datetime.date(2023, 1, 2), Amount=0.8360337791425916, ProductCategory='Groceries'),\n",
       " Row(CustomerID='C00081', TransactionID='T10025', TransactionDate=datetime.date(2023, 3, 8), Amount=0.9513717725968727, ProductCategory='Furniture'),\n",
       " Row(CustomerID='C00031', TransactionID='T10027', TransactionDate=datetime.date(2023, 6, 23), Amount=0.5048082750818215, ProductCategory='Electronics'),\n",
       " Row(CustomerID='C00020', TransactionID='T10029', TransactionDate=datetime.date(2023, 4, 12), Amount=0.8559941815830943, ProductCategory='Books')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a7259a2d-0292-4bd9-8859-dad4723f7ad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[CustomerID: string, TransactionID: string, TransactionDate: date, Amount: double, ProductCategory: string]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7582a1-b678-4ccf-b731-136fec96ad5f",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "https://spark.apache.org/docs/latest/api/python/getting_started/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8b3172-30f1-4fc7-a361-4332dbcf7638",
   "metadata": {},
   "source": [
    "## Why to Use PySpark\n",
    "Pandas has limitations such as\n",
    "1. Pandas loads entire datasets into memory so datasets larger than 2-3 GB memory RAM can cause memory errors or slowdowns.\n",
    "2. Single threaded run on a single CPU\n",
    "3. Slow on large data\n",
    "\n",
    "PySpark\n",
    "1. Perform scalable parallele data processing\n",
    "2. Integration with big data tools like Hadoop or cloud storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65a94cd-95d5-4d29-bc00-174056fbe6f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
